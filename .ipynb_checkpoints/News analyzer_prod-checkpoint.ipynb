{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os.path\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import *\n",
    "from gensim import similarities\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.matutils import softcossim\n",
    "\n",
    "\n",
    "from GoogleNews import GoogleNews\n",
    "googlenews = GoogleNews()\n",
    "\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import fulltext\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import docx\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "\n",
    "from parsers import Google_parser as gp\n",
    "\n",
    "\n",
    "## запрос по api\n",
    "import requests\n",
    "import json\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "import gensim.downloader as api\n",
    "# Download the models\n",
    "#fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "#glove_model300 = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "from emailing import sendmailto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок предобработки текста\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stop_words, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return words #' '.join(words) #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста\n",
    "\n",
    "def add_hyperlink(paragraph, url, text):\n",
    "    \"\"\"\n",
    "    A function that places a hyperlink within a paragraph object.\n",
    "    :param paragraph: The paragraph we are adding the hyperlink to.\n",
    "    :param url: A string containing the required url\n",
    "    :param text: The text displayed for the url\n",
    "    :return: The hyperlink object\n",
    "    \"\"\"\n",
    "\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "\n",
    "    # Create a new w:rPr element\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    paragraph._p.append(hyperlink)\n",
    "\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# слова-исключения, по которым будут отфильтровываться статьи\n",
    "exclude_list = ['macbook','laptop','ultrabook','server', 'api', 'usb', 'installer', 'ubuntu', 'ip', 'lan','javascript', 'captcha', 'browser','internet', 'usb-c']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выгрузки заголовков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_topic_list = ['maritime transport', 'port freight', 'container ship', 'bulk carrier', 'tanker', 'TEU', 'auto industry', 'refrigerated ship', 'reefer', 'supertanker', 'cargo ship']\n",
    "\n",
    "agro_topic_list = ['china +red +line', 'china agriculture food subsidy', 'china +gmo', 'china food reserve', 'china agriculture policy', 'india fertilizer', 'monsoon el nino drought', 'brazil inflation real', \n",
    "              'argentina export tariff',  'EU food quota']\n",
    "\n",
    "itt_topic_list = ['semiconductors market', '+software +market +volume', 'telecommunications market volume', 'iaas | paas | saas', '+it +market', 'computer sales', 'IT coronavirus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# слова-исключения, по которым будут отфильтровываться статьи\n",
    "exclude_list = ['macbook','laptop','ultrabook','server', 'api', 'usb', 'installer', 'ubuntu', 'ip', 'lan','javascript', 'captcha', 'browser','internet', 'usb-c', 'nintendo', 'docker', 'desktop', 'spacex', 'windows']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_news_list_aggregate(topiclist, topicname):\n",
    "    # задаем клиентский апи\n",
    "    newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "    # формируем границы периода времени, за которое будем запрашивать новости\n",
    "    current_date = datetime.today()\n",
    "    previous_day = current_date - timedelta(days = 1)\n",
    "    current_date = current_date.strftime('%Y-%m-%d')\n",
    "    previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "    articles_base_google = {}\n",
    "    articles_base_newsapi = {}\n",
    "    articles_base = {}\n",
    "    for topic in topiclist:\n",
    "        # выгрузка статей по одному запросу\n",
    "        all_articles = []\n",
    "        googlenews.clear()\n",
    "        googlenews.search(topic)\n",
    "        for item in googlenews.result()[0:5]:\n",
    "            all_articles.append(item)\n",
    "\n",
    "        articles_base_google[topic] = all_articles\n",
    "\n",
    "\n",
    "\n",
    "        # получение данных по теме\n",
    "        all_articles = newsapi.get_everything(q=topic,\n",
    "                                              from_param=previous_day,\n",
    "                                              to=current_date,\n",
    "                                              language='en',\n",
    "                                              sort_by='relevancy',\n",
    "                                              page_size=5)\n",
    "\n",
    "\n",
    "        articles_base_newsapi[topic] = all_articles['articles']\n",
    "\n",
    "        articles_base[topic] = articles_base_google[topic] + articles_base_newsapi[topic]\n",
    "\n",
    "    #формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "    document = Document()\n",
    "\n",
    "    doc_header = 'Глобальные новости' + '\\n' + 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "    document.add_heading(doc_header)\n",
    "\n",
    "    for topic in articles_base:\n",
    "        count = 1\n",
    "        header = 'Новости по запросу: ' + str(topic)\n",
    "        document.add_heading(header, level = 2)\n",
    "        for item in articles_base[topic]:\n",
    "            tokens = prepare_text(item['title'])\n",
    "            raise_flag_excludeword = 0\n",
    "            for token in tokens:\n",
    "                if token in exclude_list:\n",
    "                    raise_flag_excludeword = 1\n",
    "            if raise_flag_excludeword == 0: \n",
    "                try:\n",
    "                    url = item['url']\n",
    "                except:\n",
    "                    url = item['link']\n",
    "                p = document.add_paragraph()\n",
    "                link_text = str(count) + '. ' + item['title'] \n",
    "                url = add_hyperlink(p, url, link_text)\n",
    "                count += 1\n",
    "\n",
    "    doc_name = 'aggregate_' + str(topicname) + '_' + str(current_date) + '.docx'\n",
    "    document.save(doc_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_aggregate(agro_topic_list, 'Agriculture')\n",
    "prepare_news_list_aggregate(itt_topic_list, 'ITT')\n",
    "prepare_news_list_aggregate(transport_topic_list, 'Transport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок транспортных новостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# transport_topiclist = [ 'maritime AND transport', 'port AND freight', '+cargo', 'transport AND delay', 'container AND ship', '+bulk', 'bulk AND carrier',\n",
    "#                        '+tanker', '+TEU', 'auto AND industry', '+freight', 'refrigerated AND ship', '+reefer', '+supertanker', 'cargo AND ship']\n",
    "\n",
    "transport_topiclist = [ '+maritime +transport', '+port +freight', '+cargo', '+container +ship', '+bulk', '+bulk +carrier',\n",
    "                       '+tanker', '+TEU', '+auto +industry', '+freight', '+refrigerated +ship', '+reefer', '+supertanker', '+cargo +ship']\n",
    "transport_all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in transport_topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    all_articles = newsapi.get_everything(q=request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=3)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(all_articles['articles'])):\n",
    "        url  = all_articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            all_articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    transport_all_articles.append(all_articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "transport_all_contents = []\n",
    "transport_all_titles = []\n",
    "transport_all_urls = []\n",
    "for article_list in transport_all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        transport_all_contents.append(item['content'])\n",
    "        transport_all_titles.append(item['title'])\n",
    "        transport_all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(transport_all_contents)):\n",
    "    try:\n",
    "        transport_all_contents[i] = summarize(text = transport_all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in transport_all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "    \n",
    "# text_0 = mydict.doc2bow(tokenized_list[22])\n",
    "# text_1 = mydict.doc2bow(tokenized_list[32])\n",
    "     \n",
    "\n",
    "\n",
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    try:\n",
    "        comparison_index = 0\n",
    "        similarity_flag = 0\n",
    "        for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "        #for comparison in bag_of_words:\n",
    "            #if text_index != comparison_index:\n",
    "            similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "            if similarity_ratio > 0.2:\n",
    "                similarity_flag = 1\n",
    "\n",
    "        if (similarity_flag == 0) and (transport_all_contents[text_index] not in unique_articles):\n",
    "            #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "            #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "            exclude_count = 0\n",
    "            tokens = prepare_text(transport_all_contents[text_index])\n",
    "            for word in tokens:\n",
    "                if word in exclude_list:\n",
    "                    exclude_count += 1\n",
    "            if exclude_count == 0:\n",
    "                unique_articles.append(transport_all_contents[text_index])\n",
    "                unique_urls.append(transport_all_urls[text_index])\n",
    "                unique_titles.append(transport_all_titles[text_index])\n",
    "            else:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "                \n",
    "        \n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости транспорта'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Ссылка на источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name_transport = 'transport_' + str(current_date) + '.docx'\n",
    "document.save(doc_name_transport)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending mail\n",
    "sendmailto('nikoda1@gmail.com', 'Global news', doc_datestamp, doc_name_transport)\n",
    "sendmailto('Kirill.nikoda@gazprombank.ru', 'Global news', doc_datestamp, doc_name_transport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок Агро новостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "topiclist = ['+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "            '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', \n",
    "             '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el AND +nino', '+usa AND +drought']\n",
    "# topiclist = ['+china AND red AND line', '+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "#             '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', '+brazil AND +inflation', '+brazil AND +real', \n",
    "#              '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el +nino', '+usa AND +drought']\n",
    "# topiclist = ['china AND land', 'china AND red AND line', 'agro AND subsidy', 'china AND gmo', 'food AND reserve', 'agriculture AND policy', 'agriculture AND tax',\n",
    "#             'china AND tax', 'food AND trade', 'india AND fertilizer', 'india AND subsidy', 'china AND subsidy', 'monsoon', 'brazil AND inflation', 'brazil AND real', \n",
    "#              'argentina AND tariff', 'argentina AND export', 'EU AND quota', 'eu AND food', 'el nino', 'usa AND drought']\n",
    "all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    articles = newsapi.get_everything(q = request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=3)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(articles['articles'])):\n",
    "        url  = articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    all_articles.append(articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "all_contents = []\n",
    "all_titles = []\n",
    "all_urls = []\n",
    "for article_list in all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        all_contents.append(item['content'])\n",
    "        all_titles.append(item['title'])\n",
    "        all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(all_contents)):\n",
    "    try:\n",
    "        all_contents[i] = summarize(text = all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "          \n",
    "\n",
    "\n",
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    comparison_index = 0\n",
    "    similarity_flag = 0\n",
    "    for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "    #for comparison in bag_of_words:\n",
    "        #if text_index != comparison_index:\n",
    "        similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "        if similarity_ratio > 0.2:\n",
    "            similarity_flag = 1\n",
    "\n",
    "    if (similarity_flag == 0) and (all_contents[text_index] not in unique_articles):\n",
    "        #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "        #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "        exclude_count = 0\n",
    "        tokens = prepare_text(all_contents[text_index])\n",
    "        for word in tokens:\n",
    "            if word in exclude_list:\n",
    "                exclude_count += 1\n",
    "        if exclude_count == 0:\n",
    "            unique_articles.append(all_contents[text_index])\n",
    "            unique_urls.append(all_urls[text_index])\n",
    "            unique_titles.append(all_titles[text_index])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #comparison_index += 1\n",
    "    #text_index += 1\n",
    "                \n",
    "        \n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости агросектора'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Ссылка на источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name_agro = 'agro_' + str(current_date) + '.docx'\n",
    "document.save(doc_name_agro)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sendmailto('snitkodv@mail.ru', 'Global news', doc_datestamp, doc_name_agro)\n",
    "sendmailto('Daria.Snitko@gazprombank.ru', 'Global news', doc_datestamp, doc_name_agro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "topiclist = ['+china' ]\n",
    "# topiclist = ['+china AND red AND line', '+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "#             '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', '+brazil AND +inflation', '+brazil AND +real', \n",
    "#              '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el +nino', '+usa AND +drought']\n",
    "# topiclist = ['china AND land', 'china AND red AND line', 'agro AND subsidy', 'china AND gmo', 'food AND reserve', 'agriculture AND policy', 'agriculture AND tax',\n",
    "#             'china AND tax', 'food AND trade', 'india AND fertilizer', 'india AND subsidy', 'china AND subsidy', 'monsoon', 'brazil AND inflation', 'brazil AND real', \n",
    "#              'argentina AND tariff', 'argentina AND export', 'EU AND quota', 'eu AND food', 'el nino', 'usa AND drought']\n",
    "all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    articles = newsapi.get_everything(q = request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=3)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(articles['articles'])):\n",
    "        url  = articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    all_articles.append(articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "all_contents = []\n",
    "all_titles = []\n",
    "all_urls = []\n",
    "for article_list in all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        all_contents.append(item['content'])\n",
    "        all_titles.append(item['title'])\n",
    "        all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(all_contents)):\n",
    "    try:\n",
    "        all_contents[i] = summarize(text = all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "          \n",
    "\n",
    "\n",
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    comparison_index = 0\n",
    "    similarity_flag = 0\n",
    "    for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "    #for comparison in bag_of_words:\n",
    "        #if text_index != comparison_index:\n",
    "        similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "        if similarity_ratio > 0.2:\n",
    "            similarity_flag = 1\n",
    "\n",
    "    if (similarity_flag == 0) and (all_contents[text_index] not in unique_articles):\n",
    "        #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "        #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "        exclude_count = 0\n",
    "        tokens = prepare_text(all_contents[text_index])\n",
    "        for word in tokens:\n",
    "            if word in exclude_list:\n",
    "                exclude_count += 1\n",
    "        if exclude_count == 0:\n",
    "            unique_articles.append(all_contents[text_index])\n",
    "            unique_urls.append(all_urls[text_index])\n",
    "            unique_titles.append(all_titles[text_index])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #comparison_index += 1\n",
    "    #text_index += 1\n",
    "                \n",
    "        \n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости энергетики'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Ссылка на источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'energy_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
