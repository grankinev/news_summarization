{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os.path\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import *\n",
    "from gensim import similarities\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.matutils import softcossim\n",
    "\n",
    "\n",
    "from GoogleNews import GoogleNews\n",
    "googlenews = GoogleNews()\n",
    "\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import fulltext\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import docx\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "\n",
    "from parsers import Google_parser as gp\n",
    "\n",
    "\n",
    "## запрос по api\n",
    "import requests\n",
    "import json\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "import gensim.downloader as api\n",
    "# Download the models\n",
    "#fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "#glove_model300 = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "from emailing import sendmailto\n",
    "\n",
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок предобработки текста\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stop_words, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return words #' '.join(words) #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста\n",
    "\n",
    "def add_hyperlink(paragraph, url, text):\n",
    "    \"\"\"\n",
    "    A function that places a hyperlink within a paragraph object.\n",
    "    :param paragraph: The paragraph we are adding the hyperlink to.\n",
    "    :param url: A string containing the required url\n",
    "    :param text: The text displayed for the url\n",
    "    :return: The hyperlink object\n",
    "    \"\"\"\n",
    "\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "\n",
    "    # Create a new w:rPr element\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    paragraph._p.append(hyperlink)\n",
    "\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# слова-исключения, по которым будут отфильтровываться статьи\n",
    "exclude_list = ['macbook','laptop','ultrabook','server', 'api', 'usb', 'installer', 'ubuntu', 'ip', 'lan','javascript', 'captcha', 'browser','internet', 'usb-c']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок транспортных новостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# transport_topiclist = [ 'maritime AND transport', 'port AND freight', '+cargo', 'transport AND delay', 'container AND ship', '+bulk', 'bulk AND carrier',\n",
    "#                        '+tanker', '+TEU', 'auto AND industry', '+freight', 'refrigerated AND ship', '+reefer', '+supertanker', 'cargo AND ship']\n",
    "\n",
    "transport_topiclist = [ '+maritime +transport', '+maritime +transport', '+cargo', '+container +ship', '+bulk', '+bulk +carrier',\n",
    "                       '+tanker', '+TEU', '+auto +industry', '+freight', '+refrigerated +ship', '+reefer', '+supertanker', '+cargo +ship']\n",
    "transport_all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in transport_topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    all_articles = newsapi.get_everything(q=request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=3)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(all_articles['articles'])):\n",
    "        url  = all_articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            all_articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    transport_all_articles.append(all_articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "transport_all_contents = []\n",
    "transport_all_titles = []\n",
    "transport_all_urls = []\n",
    "for article_list in transport_all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        transport_all_contents.append(item['content'])\n",
    "        transport_all_titles.append(item['title'])\n",
    "        transport_all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(transport_all_contents)):\n",
    "    try:\n",
    "        transport_all_contents[i] = summarize(text = transport_all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in transport_all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "    \n",
    "# text_0 = mydict.doc2bow(tokenized_list[22])\n",
    "# text_1 = mydict.doc2bow(tokenized_list[32])\n",
    "     \n",
    "\n",
    "\n",
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    comparison_index = 0\n",
    "    similarity_flag = 0\n",
    "    for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "    #for comparison in bag_of_words:\n",
    "        #if text_index != comparison_index:\n",
    "        similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "        if similarity_ratio > 0.2:\n",
    "            similarity_flag = 1\n",
    "\n",
    "    if (similarity_flag == 0) and (transport_all_contents[text_index] not in unique_articles):\n",
    "        #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "        #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "        exclude_count = 0\n",
    "        tokens = prepare_text(transport_all_contents[text_index])\n",
    "        for word in tokens:\n",
    "            if word in exclude_list:\n",
    "                exclude_count += 1\n",
    "        if exclude_count == 0:\n",
    "            unique_articles.append(transport_all_contents[text_index])\n",
    "            unique_urls.append(transport_all_urls[text_index])\n",
    "            unique_titles.append(transport_all_titles[text_index])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #comparison_index += 1\n",
    "    #text_index += 1\n",
    "                \n",
    "        \n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости транспорта'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Ссылка на источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'transport_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in mydict.token2id.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# transport_topiclist = [ 'maritime AND transport', 'port AND freight', '+cargo', 'transport AND delay', 'container AND ship', '+bulk', 'bulk AND carrier',\n",
    "#                        '+tanker', '+TEU', 'auto AND industry', '+freight', 'refrigerated AND ship', '+reefer', '+supertanker', 'cargo AND ship']\n",
    "\n",
    "transport_topiclist = [ '+maritime +transport', '+port +freight', '+cargo', '+container +ship', '+bulk', '+bulk +carrier',\n",
    "                       '+tanker', '+TEU', '+auto +industry', '+freight', '+refrigerated +ship', '+reefer', '+supertanker', '+cargo +ship']\n",
    "transport_all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in transport_topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    all_articles = newsapi.get_everything(q=request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=3)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(all_articles['articles'])):\n",
    "        url  = all_articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            all_articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    transport_all_articles.append(all_articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "transport_all_contents = []\n",
    "transport_all_titles = []\n",
    "transport_all_urls = []\n",
    "for article_list in transport_all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        transport_all_contents.append(item['content'])\n",
    "        transport_all_titles.append(item['title'])\n",
    "        transport_all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(transport_all_contents)):\n",
    "    try:\n",
    "        transport_all_contents[i] = summarize(text = transport_all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in transport_all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция предобработки\n",
    "def prepare_text(text, stop = stop_words, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return    words#' '.join(words) #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста\n",
    "\n",
    "tokenized_list = []\n",
    "for doc in transport_all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "model = TfidfModel(mycorpus)  # fit model\n",
    "vector = model[mycorpus[0]]\n",
    "corpus_tfidf = model[mycorpus] #apply transformation to the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(tokenized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок Агро новостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "topiclist = ['+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "            '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', \n",
    "             '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el AND +nino', '+usa AND +drought']\n",
    "# topiclist = ['+china AND red AND line', '+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "#             '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', '+brazil AND +inflation', '+brazil AND +real', \n",
    "#              '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el +nino', '+usa AND +drought']\n",
    "# topiclist = ['china AND land', 'china AND red AND line', 'agro AND subsidy', 'china AND gmo', 'food AND reserve', 'agriculture AND policy', 'agriculture AND tax',\n",
    "#             'china AND tax', 'food AND trade', 'india AND fertilizer', 'india AND subsidy', 'china AND subsidy', 'monsoon', 'brazil AND inflation', 'brazil AND real', \n",
    "#              'argentina AND tariff', 'argentina AND export', 'EU AND quota', 'eu AND food', 'el nino', 'usa AND drought']\n",
    "all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    articles = newsapi.get_everything(q = request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=3)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(articles['articles'])):\n",
    "        url  = articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    all_articles.append(articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "all_contents = []\n",
    "all_titles = []\n",
    "all_urls = []\n",
    "for article_list in all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        all_contents.append(item['content'])\n",
    "        all_titles.append(item['title'])\n",
    "        all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(all_contents)):\n",
    "    try:\n",
    "        all_contents[i] = summarize(text = all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "          \n",
    "\n",
    "\n",
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    comparison_index = 0\n",
    "    similarity_flag = 0\n",
    "    for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "    #for comparison in bag_of_words:\n",
    "        #if text_index != comparison_index:\n",
    "        similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "        if similarity_ratio > 0.2:\n",
    "            similarity_flag = 1\n",
    "\n",
    "    if (similarity_flag == 0) and (all_contents[text_index] not in unique_articles):\n",
    "        #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "        #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "        exclude_count = 0\n",
    "        tokens = prepare_text(all_contents[text_index])\n",
    "        for word in tokens:\n",
    "            if word in exclude_list:\n",
    "                exclude_count += 1\n",
    "        if exclude_count == 0:\n",
    "            unique_articles.append(all_contents[text_index])\n",
    "            unique_urls.append(all_urls[text_index])\n",
    "            unique_titles.append(all_titles[text_index])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #comparison_index += 1\n",
    "    #text_index += 1\n",
    "                \n",
    "        \n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости агросектора'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Ссылка на источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'agro_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "topiclist = ['+energy AND +belarus', '+energy AND +kazakhstan', '+energy AND +uzbekistan','+renewable AND +energy' ]\n",
    "# topiclist = ['+china AND red AND line', '+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "#             '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', '+brazil AND +inflation', '+brazil AND +real', \n",
    "#              '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el +nino', '+usa AND +drought']\n",
    "# topiclist = ['china AND land', 'china AND red AND line', 'agro AND subsidy', 'china AND gmo', 'food AND reserve', 'agriculture AND policy', 'agriculture AND tax',\n",
    "#             'china AND tax', 'food AND trade', 'india AND fertilizer', 'india AND subsidy', 'china AND subsidy', 'monsoon', 'brazil AND inflation', 'brazil AND real', \n",
    "#              'argentina AND tariff', 'argentina AND export', 'EU AND quota', 'eu AND food', 'el nino', 'usa AND drought']\n",
    "all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    articles = newsapi.get_everything(q = request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=3)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(articles['articles'])):\n",
    "        url  = articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    all_articles.append(articles)\n",
    "#     #сохранение новостей в базу\n",
    "\n",
    "#     save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "#     new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "#     with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "#         print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "\n",
    "#здесь создаем три отдельных списка для текстов новостей, заголовков, ссылок. заполняем их\n",
    "all_contents = []\n",
    "all_titles = []\n",
    "all_urls = []\n",
    "for article_list in all_articles:\n",
    "    i = 0\n",
    "    for item in article_list['articles']:\n",
    "        all_contents.append(item['content'])\n",
    "        all_titles.append(item['title'])\n",
    "        all_urls.append(item['url'])\n",
    "\n",
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(all_contents)):\n",
    "    try:\n",
    "        all_contents[i] = summarize(text = all_contents[i], word_count = 100)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#формируем список списков из предобработанных саммари, разделенных на токены\n",
    "#при формировании будем проверять наличие слов-исключений, связанных с посторонними темами.\n",
    "error_index = 0\n",
    "error_list = []\n",
    "tokenized_list = []\n",
    "for doc in all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "          \n",
    "\n",
    "\n",
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    comparison_index = 0\n",
    "    similarity_flag = 0\n",
    "    for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "    #for comparison in bag_of_words:\n",
    "        #if text_index != comparison_index:\n",
    "        similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "        if similarity_ratio > 0.2:\n",
    "            similarity_flag = 1\n",
    "\n",
    "    if (similarity_flag == 0) and (all_contents[text_index] not in unique_articles):\n",
    "        #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "        #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "        exclude_count = 0\n",
    "        tokens = prepare_text(all_contents[text_index])\n",
    "        for word in tokens:\n",
    "            if word in exclude_list:\n",
    "                exclude_count += 1\n",
    "        if exclude_count == 0:\n",
    "            unique_articles.append(all_contents[text_index])\n",
    "            unique_urls.append(all_urls[text_index])\n",
    "            unique_titles.append(all_titles[text_index])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #comparison_index += 1\n",
    "    #text_index += 1\n",
    "                \n",
    "        \n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости агросектора'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Ссылка на источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'agro_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем клиентский апи\n",
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "topiclist = ['+energy AND +belarus', '+energy AND +kazakhstan', '+energy AND +uzbekistan','+renewable AND +energy' ]\n",
    "# topiclist = ['+china AND red AND line', '+agriculture AND +subsidy', '+china AND +gmo', '+agriculture AND +policy', '+agriculture AND +tax',\n",
    "#             '+food AND +trade', '+india AND +fertilizer', '+india AND +subsidy', '+china AND +subsidy', '+monsoon', '+brazil AND +inflation', '+brazil AND +real', \n",
    "#              '+argentina AND +tariff', '+argentina AND +export', '+EU AND +quota', '+el +nino', '+usa AND +drought']\n",
    "# topiclist = ['china AND land', 'china AND red AND line', 'agro AND subsidy', 'china AND gmo', 'food AND reserve', 'agriculture AND policy', 'agriculture AND tax',\n",
    "#             'china AND tax', 'food AND trade', 'india AND fertilizer', 'india AND subsidy', 'china AND subsidy', 'monsoon', 'brazil AND inflation', 'brazil AND real', \n",
    "#              'argentina AND tariff', 'argentina AND export', 'EU AND quota', 'eu AND food', 'el nino', 'usa AND drought']\n",
    "all_articles = []\n",
    "body_fulltext = []\n",
    "\n",
    "for topic in topiclist:\n",
    "    #задаем тему запроса\n",
    "    request_topic = topic\n",
    "\n",
    "    # получение данных по теме\n",
    "    articles = newsapi.get_everything(q = request_topic,\n",
    "                                          from_param=previous_day,\n",
    "                                          to=current_date,\n",
    "                                          language='en',\n",
    "                                          sort_by='relevancy',\n",
    "                                          page_size=5)\n",
    "    \n",
    "    #Формирование служебных переменных\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "    \n",
    "\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    #попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "    for i in range(0, len(articles['articles'])):\n",
    "        url  = articles['articles'][i]['url']\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body_fulltext.append(text) \n",
    "            articles['articles'][i]['content'] = text \n",
    "        except:\n",
    "            error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "    \n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    all_articles.append(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(mycorpus, smartirs='ntc')\n",
    "for doc in tfidf[mycorpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = word2vec_model300.similarity_matrix(mydict, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))\n",
    "    \n",
    "# text_0 = mydict.doc2bow(tokenized_list[22])\n",
    "# text_1 = mydict.doc2bow(tokenized_list[32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# слова-исключения, по которым будут отфильтровываться статьи\n",
    "exclude_list = ['macbook','laptop','ultrabook','server', 'api', 'usb', 'installer', 'ubuntu', 'ip', 'lan','javascript', 'captcha', 'browser','internet', 'usb-c']        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# производится проверка на наличие схожих текстов. \n",
    "# если степень схожется меньше 0.5 проверяем на наличие текста в списке уникальных\n",
    "# добавляем в список уникальных при условии несхожести и отсутствия дублирования\n",
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    comparison_index = 0\n",
    "    similarity_flag = 0\n",
    "    for comparison_index in range(text_index+1, len(bag_of_words)):\n",
    "    #for comparison in bag_of_words:\n",
    "        #if text_index != comparison_index:\n",
    "        similarity_ratio = softcossim(bag_of_words[text_index], bag_of_words[comparison_index], similarity_matrix)\n",
    "        if similarity_ratio > 0.2:\n",
    "            similarity_flag = 1\n",
    "\n",
    "    if (similarity_flag == 0) and (transport_all_contents[text_index] not in unique_articles):\n",
    "        #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "        #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "        exclude_count = 0\n",
    "        tokens = prepare_text(transport_all_contents[text_index])\n",
    "        for word in tokens:\n",
    "            if word in exclude_list:\n",
    "                exclude_count += 1\n",
    "        if exclude_count == 0:\n",
    "            unique_articles.append(transport_all_contents[text_index])\n",
    "            unique_urls.append(transport_all_urls[text_index])\n",
    "            unique_titles.append(transport_all_titles[text_index])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        #comparison_index += 1\n",
    "    #text_index += 1\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hyperlink(paragraph, url, text):\n",
    "    \"\"\"\n",
    "    A function that places a hyperlink within a paragraph object.\n",
    "    :param paragraph: The paragraph we are adding the hyperlink to.\n",
    "    :param url: A string containing the required url\n",
    "    :param text: The text displayed for the url\n",
    "    :return: The hyperlink object\n",
    "    \"\"\"\n",
    "\n",
    "    # This gets access to the document.xml.rels file and gets a new relation id value\n",
    "    part = paragraph.part\n",
    "    r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)\n",
    "\n",
    "    # Create the w:hyperlink tag and add needed values\n",
    "    hyperlink = docx.oxml.shared.OxmlElement('w:hyperlink')\n",
    "    hyperlink.set(docx.oxml.shared.qn('r:id'), r_id, )\n",
    "\n",
    "    # Create a w:r element\n",
    "    new_run = docx.oxml.shared.OxmlElement('w:r')\n",
    "\n",
    "    # Create a new w:rPr element\n",
    "    rPr = docx.oxml.shared.OxmlElement('w:rPr')\n",
    "\n",
    "    # Join all the xml elements together add add the required text to the w:r element\n",
    "    new_run.append(rPr)\n",
    "    new_run.text = text\n",
    "    hyperlink.append(new_run)\n",
    "\n",
    "    paragraph._p.append(hyperlink)\n",
    "\n",
    "    return hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Новости транспорта'#'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(unique_articles)):\n",
    "    #try:\n",
    "        header = unique_titles[i]\n",
    "        document.add_heading(header, level = 1)\n",
    "        content = unique_articles[i]\n",
    "        document.add_paragraph(content)\n",
    "        url = unique_urls[i]\n",
    "        p=document.add_paragraph()\n",
    "        #p = document.add_paragraph()\n",
    "        url = add_hyperlink(p, url, 'Источник')\n",
    "#         run = document.add_paragraph().add_run()\n",
    "#         font = run.font\n",
    "#         font.color.rgb = RGBColor(0x42, 0x24, 0xE9)\n",
    "#         p=document.add_paragraph()\n",
    "#         url = unique_urls[i]\n",
    "#         document.add_paragraph(url)\n",
    "        \n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'transport_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model300.most_similar('transport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #формируем документ из коротких саммари новостей\n",
    "    document = Document()\n",
    "    doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "    doc_header = 'Тематика новостей: ' + str(request_topic)\n",
    "    document.add_heading(doc_header)\n",
    "    document.add_heading(doc_datestamp, level = 1)\n",
    "    for i in range (0, len(all_articles['articles'])):\n",
    "        header = all_articles['articles'][i]['title']\n",
    "        document.add_heading(header, level = 1)\n",
    "        try:\n",
    "            content = summarize(text = all_articles['articles'][i]['content'], word_count= 100)\n",
    "        except:\n",
    "            content = all_articles['articles'][i]['content']\n",
    "        document.add_paragraph(content)\n",
    "        url = all_articles['articles'][i]['url']\n",
    "        document.add_paragraph(url)\n",
    "\n",
    "    #doc_name = str(request_topic) + '.docx' \n",
    "    doc_name = str(request_topic) + '_summary100.docx'\n",
    "    document.save(doc_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### запрос по всей базе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "request_topic = 'china'\n",
    "\n",
    "# получение данных по теме\n",
    "all_articles = newsapi.get_everything(q=request_topic,\n",
    "                                      from_param=previous_day,\n",
    "                                      to=current_date,\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page_size=10)\n",
    "\n",
    "#Формирование служебных переменных\n",
    "count = 0\n",
    "error_link = [] #массив с битыми ссылками\n",
    "\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "#попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "for i in range(0, len(all_articles['articles'])):\n",
    "    url  = all_articles['articles'][i]['url']\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body_fulltext.append(text) \n",
    "        all_articles['articles'][i]['content'] = text \n",
    "    except:\n",
    "        error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "        all_articles['articles'][i]['content'] = all_articles['articles'][i]['content'][0:258]#если статью не удалось выгрузить, поле COntent будем заполнять 259 знаками из самого описания\n",
    "        pass\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "\n",
    "#сохранение новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "    print('Выгрузка новостей сохранена в файл')\n",
    "\n",
    "#формируем документ из коротких саммари новостей\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(all_articles['articles'])):\n",
    "    header = all_articles['articles'][i]['title']\n",
    "    document.add_heading(header, level = 1)\n",
    "    try:\n",
    "        content = summarize(text = all_articles['articles'][i]['content'], word_count= 100)\n",
    "    except:\n",
    "        content = all_articles['articles'][i]['content']\n",
    "    document.add_paragraph(content)\n",
    "    url = all_articles['articles'][i]['url']\n",
    "    document.add_paragraph(url)\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = str(request_topic) + '_summary100.docx'\n",
    "document.save(doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запрос по конкретной стране"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_topic = 'transportation'\n",
    "\n",
    "# получение данных по теме\n",
    "top_headlines = newsapi.get_top_headlines(q=request_topic,\n",
    "                                          #category='business',\n",
    "                                          language='en',\n",
    "                                          #country='cn')\n",
    "                                         )\n",
    "\n",
    "top_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_headlines = newsapi.get_top_headlines(q=request_topic,\n",
    "                                          #category='business',\n",
    "                                          language='en',\n",
    "                                          country='us')\n",
    "top_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "request_topic = 'china land reform'\n",
    "\n",
    "# получение данных по теме\n",
    "top_headlines = newsapi.get_top_headlines(q=request_topic,\n",
    "                                          category='business',\n",
    "                                          language='en',\n",
    "                                          pageSize = 10,\n",
    "                                          country='cn')\n",
    "\n",
    "#Формирование служебных переменных\n",
    "count = 0\n",
    "error_link = [] #массив с битыми ссылками\n",
    "\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "#попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "for i in range(0, len(all_articles['articles'])):\n",
    "    url  = all_articles['articles'][i]['url']\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body_fulltext.append(text) \n",
    "        all_articles['articles'][i]['content'] = text \n",
    "    except:\n",
    "        error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "        all_articles['articles'][i]['content'] = all_articles['articles'][i]['content'][0:258]#если статью не удалось выгрузить, поле COntent будем заполнять 259 знаками из самого описания\n",
    "        pass\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        print(count)\n",
    "\n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "\n",
    "#сохранение новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "    print('Выгрузка новостей сохранена в файл')\n",
    "\n",
    "#формируем документ из коротких саммари новостей\n",
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(all_articles['articles'])):\n",
    "    header = all_articles['articles'][i]['title']\n",
    "    document.add_heading(header, level = 1)\n",
    "    try:\n",
    "        content = summarize(text = all_articles['articles'][i]['content'], word_count= 100)\n",
    "    except:\n",
    "        content = all_articles['articles'][i]['content']\n",
    "    document.add_paragraph(content)\n",
    "    url = all_articles['articles'][i]['url']\n",
    "    document.add_paragraph(url)\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = str(request_topic) + '_summary100.docx'\n",
    "document.save(doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Формирование массива ссылок из выдачи Api\n",
    "count = 0\n",
    "error_link = [] #массив с битыми ссылками\n",
    "\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "#сохраним копию выгрузки с коротким содержанием новостей\n",
    "all_articles_short = all_articles.copy()\n",
    "\n",
    "#попытаемся скачать текст новости из первоисточника, если не получится - оставим краткое описание\n",
    "for i in range(0, len(all_articles['articles'])):\n",
    "    url  = all_articles['articles'][i]['url']\n",
    "    gnews_links.append(url)\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body_fulltext.append(text) \n",
    "        all_articles['articles'][i]['content'] = text \n",
    "    except:\n",
    "        error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "        all_articles['articles'][i]['content'] = all_articles['articles'][i]['content'][0:258]#если статью не удалось выгрузить, поле COntent будем заполнять 259 знаками из самого описания\n",
    "        pass\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        print(count)\n",
    "    \n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "\n",
    "#сохранение новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + str(request_topic) + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "    print('Выгрузка новостей сохранена в файл')\n",
    "    \n",
    "document = Document()\n",
    "document.add_heading(request_topic)\n",
    "for i in range (0, len(all_articles['articles'])):\n",
    "    header = all_articles['articles'][i]['title']\n",
    "    document.add_heading(header, level = 1)\n",
    "    content = summarize(text = all_articles['articles'][i]['content'], word_count= 100)\n",
    "    #all_articles['articles'][i]['content']\n",
    "    document.add_paragraph(content)\n",
    "    url = all_articles['articles'][i]['url']\n",
    "    document.add_paragraph(url)\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = str(request_topic) + '_summary100.docx'\n",
    "document.save(doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = Document()\n",
    "doc_datestamp = 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "doc_header = 'Тематика новостей: ' + str(request_topic)\n",
    "document.add_heading(doc_header)\n",
    "document.add_heading(doc_datestamp, level = 1)\n",
    "for i in range (0, len(all_articles['articles'])):\n",
    "    header = all_articles['articles'][i]['title']\n",
    "    document.add_heading(header, level = 1)\n",
    "    content = summarize(text = all_articles['articles'][i]['content'], word_count= 100)\n",
    "    #all_articles['articles'][i]['content']\n",
    "    document.add_paragraph(content)\n",
    "    url = all_articles['articles'][i]['url']\n",
    "    document.add_paragraph(url)\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = str(request_topic) + '_summary100.docx'\n",
    "document.save(doc_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок выгрузки из ГУГЛа и предобработки новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_topiclist_rus = ['Avtovaz', 'Lada', 'Kia', 'Hyundai ', 'Avtodizel', 'GAZ', 'Gazel', 'KAMAZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция предобработки русскоязычных текстов\n",
    "from nltk.corpus import stopwords\n",
    "stop_rus = stopwords.words('russian')\n",
    "\n",
    "def prepare_text_russian(text, stop = stop_rus, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return    words#' '.join(words) #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция предобработки\n",
    "def prepare_text(text, stop = stop_words, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return    words#' '.join(words) #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def googleparser (topic, depth):\n",
    "    '''\n",
    "    topic - тема новостного запроса\n",
    "    depth - число опрашиваемых страниц поисковой выдачи\n",
    "    \n",
    "    Возвращает два списка: с текстами новостей и с проблемными ссылками \n",
    "    \n",
    "    '''\n",
    "    # 1. формирование массива ссылок\n",
    "    gnews_links = []\n",
    "    gnews=[]\n",
    "    googlenews.search(topic)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('--- Формируется массив ссылок... ---')\n",
    "\n",
    "    for i in range(1,depth):\n",
    "        googlenews.clear()\n",
    "        googlenews.getpage(i)\n",
    "        for j in range(0, len(googlenews.gettext())):\n",
    "            gnews.append(googlenews.gettext()[j])\n",
    "            gnews_links.append(googlenews.getlinks()[j])\n",
    "\n",
    "    print(\"--- На формирование массива затрачено %s секунд ---\" % (time.time() - start_time))             \n",
    "    print('--- Завершено. Получено %s ссылок ---' % len(gnews_links))   \n",
    "    \n",
    "    # 2. выгрузка новостей и формирование массива текстов\n",
    "    \n",
    "    body = []\n",
    "    count = 0\n",
    "    error_link = [] #массив с битыми ссылками\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('--- Выгружаются новости... ---')\n",
    "\n",
    "    for url in gnews_links:\n",
    "        try:\n",
    "            html = requests.get(url).text\n",
    "            text = fulltext(html)\n",
    "            body.append(text)\n",
    "        except:\n",
    "            error_link.append(gnews_links[count]) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "            pass\n",
    "        count += 1\n",
    "        if count % 5 == 0:\n",
    "            print(count)\n",
    "\n",
    "    print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))             \n",
    "    return body, error_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews = GoogleNews('ru')\n",
    "topic = transport_topiclist_rus[0]\n",
    "googlenews.search(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Будем запрашивать новости из гугла на русском языке транслитом (т.к. модуль почему-то не работает с выгрузками на англ)\n",
    "googlenews = GoogleNews('ru')\n",
    "all_articles = []\n",
    "for topic in transport_topiclist_rus:\n",
    "    googlenews.clear()\n",
    "    googlenews.search(topic)\n",
    "    for item in googlenews.result():\n",
    "        all_articles.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем массивы с данными о заглавиях, ссылках и содержании новостей\n",
    "all_contents = []\n",
    "all_titles = []\n",
    "all_urls = []\n",
    "\n",
    "for article_number in range(0,len(all_articles)):\n",
    "    url  = all_articles[article_number]['link']\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html, language = 'ru')\n",
    "    except:\n",
    "        text = all_articles[article_number]['desc']\n",
    "    if text != '':\n",
    "        all_contents.append(text)\n",
    "        all_titles.append(all_articles[article_number]['title'])\n",
    "        all_urls.append(url)   \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_text(all_contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем можно ли записать вместо текстов саммари\n",
    "for i in range (0, len(all_contents)):\n",
    "    try:\n",
    "        summary = summarize(text = all_contents[i], word_count = 100)\n",
    "        if len(summary) > 0:\n",
    "            all_contents[i] = summary\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_list = []\n",
    "for doc in all_contents:\n",
    "    try:\n",
    "        tokens = prepare_text(doc)\n",
    "        tokenized_list.append(tokens)\n",
    "        error_index += 1\n",
    "    except:\n",
    "        error_list.append(error_index)\n",
    "        error_index += 1\n",
    "#отправляем это хозяйство в словарь и в корпус\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec, WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix\n",
    "from gensim.models.keyedvectors import WordEmbeddingSimilarityIndex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем матрицу схожести на основе модели Word2vec\n",
    "#здесь считается косинусная близость для всех пар векторов (слов)\n",
    "termsim_index = WordEmbeddingSimilarityIndex(word2vec_model300)\n",
    "similarity_matrix = SparseTermSimilarityMatrix(termsim_index, mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# конвертируем тексты новостей в bagofwords и создаем массив из списков bow\n",
    "bag_of_words = []\n",
    "for text in tokenized_list:\n",
    "    bag_of_words.append(mydict.doc2bow(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считаем косинусную близость между всеми документами\n",
    "docsim_index = SoftCosineSimilarity(bag_of_words, similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест выгрузок по одной теме за раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_topic_list = ['maritime transport', 'port freight', 'container ship', 'bulk carrier', 'tanker', 'TEU', 'auto industry', 'refrigerated ship', 'reefer', 'supertanker', 'cargo ship']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# слова-исключения, по которым будут отфильтровываться статьи\n",
    "exclude_list = ['macbook','laptop','ultrabook','server', 'api', 'usb', 'installer', 'ubuntu', 'ip', 'lan','javascript', 'captcha', 'browser','internet', 'usb-c', 'nintendo', 'docker', 'desktop', 'spacex', 'windows']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agro_topic_list = ['+china AND (+red +line)', '+china +((agriculture | food) AND subsidy)', '+china +gmo', '+china +(food AND reserve)', '+china +(agriculture AND policy)', '+india AND +fertilizer', '+monsoon | +(el AND nino) | (+USA  +drought)', 'brazil AND (inflation | real)', \n",
    "#              '+argentina +(export AND tariff)',  '+EU | +Europe +(food AND quota)']\n",
    "agro_topic_list = ['china +red +line', 'china agriculture food subsidy', 'china +gmo', 'china food reserve', 'china agriculture policy', 'india fertilizer', 'monsoon el nino drought', 'brazil inflation real', \n",
    "              'argentina export tariff',  'EU food quota']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "itt_topic_list = ['semiconductors market', '+software +market +volume', 'telecommunications market volume', 'iaas | paas | saas', '+it +market', 'computer sales', 'IT coronavirus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_news_list_google(topiclist, topicname):\n",
    "    articles_base = {}\n",
    "    for topic in topiclist:\n",
    "        # выгрузка статей по одному запросу\n",
    "        all_articles = []\n",
    "        googlenews.clear()\n",
    "        googlenews.search(topic)\n",
    "        for item in googlenews.result()[0:5]:\n",
    "            all_articles.append(item)\n",
    "            \n",
    "        #dict of topics and data for each topic\n",
    "\n",
    "        articles_base[topic] = all_articles\n",
    "        \n",
    "    # формируем границы периода времени, за которое будем запрашивать новости\n",
    "    current_date = datetime.today()\n",
    "    previous_day = current_date - timedelta(days = 1)\n",
    "    current_date = current_date.strftime('%Y-%m-%d')\n",
    "    previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "    #формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "    document = Document()\n",
    "\n",
    "    doc_header = 'Глобальные новости' + '\\n' + 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "    document.add_heading(doc_header)\n",
    "    \n",
    "    for topic in articles_base:\n",
    "        count = 1\n",
    "        header = 'Новости по запросу: ' + str(topic)\n",
    "        document.add_heading(header, level = 2)\n",
    "        for item in articles_base[topic]:\n",
    "            tokens = prepare_text(item['title'])\n",
    "            raise_flag_excludeword = 0\n",
    "            for token in tokens:\n",
    "                if token in exclude_list:\n",
    "                    raise_flag_excludeword = 1\n",
    "            if raise_flag_excludeword == 0:         \n",
    "                url = item['link']\n",
    "                p = document.add_paragraph()\n",
    "                link_text = str(count) + '. ' + item['title'] \n",
    "                url = add_hyperlink(p, url, link_text)\n",
    "                count += 1\n",
    "\n",
    "    #doc_name = str(request_topic) + '.docx' \n",
    "    doc_name = str(topicname) + '_' + str(current_date) + '.docx'\n",
    "    document.save(doc_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_news_list_newsapi(topiclist, topicname):\n",
    "    articles_base = {}\n",
    "    for topic in topiclist:\n",
    "\n",
    "        \n",
    "        # формируем границы периода времени, за которое будем запрашивать новости\n",
    "        current_date = datetime.today()\n",
    "        previous_day = current_date - timedelta(days = 1)\n",
    "        current_date = current_date.strftime('%Y-%m-%d')\n",
    "        previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "        # получение данных по теме\n",
    "        all_articles = newsapi.get_everything(q=topic,\n",
    "                                              from_param=previous_day,\n",
    "                                              to=current_date,\n",
    "                                              language='en',\n",
    "                                              sort_by='relevancy',\n",
    "                                              page_size=10)\n",
    "\n",
    "\n",
    "        articles_base[topic] = all_articles['articles']\n",
    "\n",
    "    #формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "    document = Document()\n",
    "\n",
    "    doc_header = 'Глобальные новости' + '\\n' + 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "    document.add_heading(doc_header)\n",
    "\n",
    "    for topic in articles_base:\n",
    "        count = 1\n",
    "        header = 'Новости по запросу: ' + str(topic)\n",
    "        document.add_heading(header, level = 2)\n",
    "        for item in articles_base[topic]:\n",
    "            tokens = prepare_text(item['title'])\n",
    "            raise_flag_excludeword = 0\n",
    "            for token in tokens:\n",
    "                if token in exclude_list:\n",
    "                    raise_flag_excludeword = 1\n",
    "            if raise_flag_excludeword == 0:         \n",
    "                url = item['url']\n",
    "                p = document.add_paragraph()\n",
    "                link_text = str(count) + '. ' + item['title'] \n",
    "                url = add_hyperlink(p, url, link_text)\n",
    "                count += 1\n",
    "\n",
    "    #doc_name = str(request_topic) + '.docx' \n",
    "    doc_name = str(topicname) + '_' + str(current_date) + '.docx'\n",
    "    document.save(doc_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_news_list_aggregate(topiclist, topicname):\n",
    "    # формируем границы периода времени, за которое будем запрашивать новости\n",
    "    current_date = datetime.today()\n",
    "    previous_day = current_date - timedelta(days = 1)\n",
    "    current_date = current_date.strftime('%Y-%m-%d')\n",
    "    previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "    articles_base_google = {}\n",
    "    articles_base_newsapi = {}\n",
    "    articles_base = {}\n",
    "    for topic in topiclist:\n",
    "        # выгрузка статей по одному запросу\n",
    "        all_articles = []\n",
    "        googlenews.clear()\n",
    "        googlenews.search(topic)\n",
    "        for item in googlenews.result()[0:5]:\n",
    "            all_articles.append(item)\n",
    "\n",
    "        articles_base_google[topic] = all_articles\n",
    "\n",
    "\n",
    "\n",
    "        # получение данных по теме\n",
    "        all_articles = newsapi.get_everything(q=topic,\n",
    "                                              from_param=previous_day,\n",
    "                                              to=current_date,\n",
    "                                              language='en',\n",
    "                                              sort_by='relevancy',\n",
    "                                              page_size=5)\n",
    "\n",
    "\n",
    "        articles_base_newsapi[topic] = all_articles['articles']\n",
    "\n",
    "        articles_base[topic] = articles_base_google[topic] + articles_base_newsapi[topic]\n",
    "\n",
    "    #формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "    document = Document()\n",
    "\n",
    "    doc_header = 'Глобальные новости' + '\\n' + 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "    document.add_heading(doc_header)\n",
    "\n",
    "    for topic in articles_base:\n",
    "        count = 1\n",
    "        header = 'Новости по запросу: ' + str(topic)\n",
    "        document.add_heading(header, level = 2)\n",
    "        for item in articles_base[topic]:\n",
    "            tokens = prepare_text(item['title'])\n",
    "            raise_flag_excludeword = 0\n",
    "            for token in tokens:\n",
    "                if token in exclude_list:\n",
    "                    raise_flag_excludeword = 1\n",
    "            if raise_flag_excludeword == 0: \n",
    "                try:\n",
    "                    url = item['url']\n",
    "                except:\n",
    "                    url = item['link']\n",
    "                p = document.add_paragraph()\n",
    "                link_text = str(count) + '. ' + item['title'] \n",
    "                url = add_hyperlink(p, url, link_text)\n",
    "                count += 1\n",
    "\n",
    "    doc_name = 'aggregate_' + str(topicname) + '_' + str(current_date) + '.docx'\n",
    "    document.save(doc_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_newsapi(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_google(itt_topic_list, 'ITT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_google(topic_list, 'Transport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_aggregate(agro_topic_list, 'Agriculture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_aggregate(topic_list, 'Transport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_news_list_aggregate(itt_topic_list, 'ITT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-134-f1f738685f11>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-134-f1f738685f11>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    )#сформируем массивы с данными о заглавиях, ссылках и содержании новостей\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#сформируем массивы с данными о заглавиях, ссылках и содержании новостей\n",
    "all_contents = []\n",
    "all_titles = []\n",
    "all_urls = []\n",
    "\n",
    "\n",
    "for article_number in range(0,len(all_articles)):\n",
    "    url  = all_articles[article_number]['link']\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html, language = 'ru')\n",
    "    except:\n",
    "        text = all_articles[article_number]['desc']\n",
    "    if text != '':\n",
    "        all_contents.append(text)\n",
    "        all_titles.append(all_articles[article_number]['title'])\n",
    "        all_urls.append(url)   \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_index = 0\n",
    "comparison_index = 0\n",
    "unique_articles = []\n",
    "unique_urls = []\n",
    "unique_titles = []\n",
    "\n",
    "for text_index in range(0, len(bag_of_words)):\n",
    "    query = tokenized_list[text_index]\n",
    "    if query != []:\n",
    "        similarity_flag = 0\n",
    "        sims = docsim_index[mydict.doc2bow(query)]\n",
    "        for comparison_index in range(text_index + 1, len(bag_of_words)):\n",
    "            similarity_ratio = sims[comparison_index]\n",
    "            if similarity_ratio > 0.2:\n",
    "                similarity_flag = 1\n",
    "\n",
    "        if (similarity_flag == 0) and (all_contents[text_index] not in unique_articles):\n",
    "            #текст потенциально подходит. проверим на наличие перекрестных тем\n",
    "            #будем считать текст не по теме, если в нем встречается 2 и более посторонних тематических слов\n",
    "            exclude_count = 0\n",
    "            tokens = prepare_text(all_contents[text_index])\n",
    "            for word in tokens:\n",
    "                if word in exclude_list:\n",
    "                    exclude_count += 1\n",
    "            if exclude_count == 0:\n",
    "                unique_articles.append(all_contents[text_index])\n",
    "                unique_urls.append(all_urls[text_index])\n",
    "                unique_titles.append(all_titles[text_index])\n",
    "            else:\n",
    "                pass\n",
    "    else:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# формируем границы периода времени, за которое будем запрашивать новости\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')\n",
    "\n",
    "#формируем документ из коротких саммари новостей отфильтрованных на различность.\n",
    "document = Document()\n",
    "\n",
    "doc_header = 'Новости транспорта' + '\\n' + 'Период: ' + str(previous_day) + ' - ' + str(current_date)\n",
    "document.add_heading(doc_header)\n",
    "\n",
    "for topic in articles_base:\n",
    "    count = 1\n",
    "    header = 'Новости по запросу: ' + str(topic)\n",
    "    document.add_heading(header, level = 2)\n",
    "    for item in articles_base[topic]:\n",
    "        tokens = prepare_text(item['title'])\n",
    "        raise_flag_excludeword = 0\n",
    "        for token in tokens:\n",
    "            if token in exclude_list:\n",
    "                raise_flag_excludeword = 1\n",
    "        if raise_flag_excludeword == 0:         \n",
    "            url = item['url']\n",
    "            p = document.add_paragraph()\n",
    "            link_text = str(count) + '. ' + item['title'] \n",
    "            url = add_hyperlink(p, url, link_text)\n",
    "            count += 1\n",
    "\n",
    "\n",
    "\n",
    "#doc_name = str(request_topic) + '.docx' \n",
    "doc_name = 'experimental_transport_' + str(current_date) + '.docx'\n",
    "document.save(doc_name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body, errors = gp.googleparser('China economy',20)\n",
    "body, errors = googleparser('Agriculture',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unlabeled_body_20200207.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    i=0\n",
    "    for item in body:\n",
    "        f.write('Story %s\\n' %(i))\n",
    "        f.write(item.replace('\\n\\n','\\n'))\n",
    "        f.write('\\n\\n')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### запрос по api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## запрос по api\n",
    "import requests\n",
    "import json\n",
    "from newsapi import NewsApiClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ('https://newsapi.org/v2/everything?'\n",
    "      'q=maritime transport&'\n",
    "      'from=2020-03-27&'\n",
    "      'sortBy=popularity&'\n",
    "      'apiKey=8a38d185855d4fbda8c437e94149807d')\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохранение выгрузки новостей в базу\n",
    "json_data_agro = json.loads(response.text)\n",
    "json_data_agro['articles']\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + 'agro' + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data_agro, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохранение выгрузки новостей в базу\n",
    "json_data_china = json.loads(response.text)\n",
    "json_data_china['articles']\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + 'china' + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data_china, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###выгрузка корпуса новостей, их сохранение и предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key='8a38d185855d4fbda8c437e94149807d')\n",
    "current_date = datetime.today()\n",
    "previous_day = current_date - timedelta(days = 1)\n",
    "current_date = current_date.strftime('%Y-%m-%d')\n",
    "previous_day = previous_day.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение данных по Китаю\n",
    "all_articles = newsapi.get_everything(q='maritime transport',\n",
    "                                      from_param=previous_day,\n",
    "                                      to=current_date,\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page_size=10)\n",
    "\n",
    "#Формирование массива ссылок из выдачи Api\n",
    "news_links = []\n",
    "body_fulltext = []\n",
    "count = 0\n",
    "error_link = [] #массив с битыми ссылками\n",
    "gnews_links = []\n",
    "gnews=[]\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "for i in range(0, len(all_articles['articles'])):\n",
    "    url  = all_articles['articles'][i]['url']\n",
    "    gnews_links.append(url)\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body_fulltext.append(text)\n",
    "        all_articles['articles'][i]['content'] = text\n",
    "    except:\n",
    "        error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "        all_articles['articles'][i]['content'] = all_articles['articles'][i]['content'][0:258]#если статью не удалось выгрузить, поле COntent будем заполнять 259 знаками из самого описания\n",
    "        pass\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        print(count)\n",
    "    \n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "\n",
    "#сохранение новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + 'maritime' + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "    print('Выгрузка новостей сохранена в файл')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение данных по миру\n",
    "all_articles = newsapi.get_everything(q='world economy',\n",
    "                                      from_param=current_date,\n",
    "                                      to=current_date,\n",
    "                                      language='en',\n",
    "                                      sort_by='relevancy',\n",
    "                                      page_size=100)\n",
    "\n",
    "#Формирование массива ссылок из выдачи Api\n",
    "news_links = []\n",
    "body_fulltext = []\n",
    "count = 0\n",
    "error_link = [] #массив с битыми ссылками\n",
    "gnews_links = []\n",
    "gnews=[]\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "for i in range(0, len(all_articles['articles'])):\n",
    "    url  = all_articles['articles'][i]['url']\n",
    "    gnews_links.append(url)\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body_fulltext.append(text)\n",
    "        all_articles['articles'][i]['content'] = text\n",
    "    except:\n",
    "        error_link.append(url) #иногда попадаются проблемные ссылки. Здесь мы будем сохранять их \n",
    "        all_articles['articles'][i]['content'] = all_articles['articles'][i]['content'][0:258]#если статью не удалось выгрузить, поле COntent будем заполнять 259 знаками из самого описания\n",
    "        pass\n",
    "    count += 1\n",
    "    if count % 5 == 0:\n",
    "        print(count)\n",
    "    \n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "\n",
    "#сохранение агро новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + 'world' + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)\n",
    "    print('Выгрузка новостей сохранена в файл')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохранение китайских новостей в базу\n",
    "\n",
    "save_path = 'C:/Users/Egran/YandexDisk/Work/news_project/news_database/'\n",
    "new_filename = save_path + 'china' + '_' + str(time.gmtime().tm_year) + '_' + str(time.gmtime().tm_mon) + '_' + str(time.gmtime().tm_mday) + '.txt'\n",
    "with open(new_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(body_fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for article in json_data_china['articles']:\n",
    "    print('article', i)\n",
    "    print(article['description'])\n",
    "    i += 1\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "body = []\n",
    "for article in json_data_ncov['articles']:\n",
    "    try:\n",
    "        print('article', i)\n",
    "        print(article['content'][0:258])\n",
    "        body.append(article['content'][0:258])\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    print('\\n')\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in json_data_ncov['articles']:\n",
    "    try:\n",
    "        print('article', i)\n",
    "        print(article['content'][0:258])\n",
    "        body.append(article['content'][0:258])\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for company in companies:\n",
    "    try:\n",
    "        body, errors = gp.googleparser(company,30)\n",
    "        with open(company + '.txt', 'w',encoding=\"utf-8\") as f:\n",
    "            i=0\n",
    "            for item in body:\n",
    "                f.write('Story %s\\n' %(i))\n",
    "                f.write(item.replace('\\n\\n','\\n'))\n",
    "                f.write('\\n\\n')\n",
    "                i += 1\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in body_fulltext:\n",
    "    prepared_texts.append(prepare_text(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Блок выгрузки линков и заголовков новостей из гугла\n",
    "#новости выгружаются по заданной теме, глубина выгрузки - 10 страниц\n",
    "\n",
    "topic = 'China'\n",
    "gnews_links = []\n",
    "gnews=[]\n",
    "googlenews.search(topic)\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "print('--- Формируется массив ссылок... ---')\n",
    "\n",
    "for i in range(1,2):\n",
    "    googlenews.clear()\n",
    "    googlenews.getpage(i)\n",
    "    for j in range(0, len(googlenews.gettext())):\n",
    "        gnews.append(googlenews.gettext()[j])\n",
    "        gnews_links.append(googlenews.getlinks()[j])\n",
    "\n",
    "print(\"--- На формирование массива затрачено %s секунд ---\" % (time.time() - start_time))             \n",
    "print('--- Заврешено. Выгружено %s статей ---' % len(gnews_links))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#блок выгрузки новостей и формирования массива текстов\n",
    "body = []\n",
    "count = 0\n",
    "error_position = [] #массив с номерами битых ссылок\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "print('--- Выгружаются новости... ---')\n",
    "\n",
    "for url in gnews_links:\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body.append(text)\n",
    "    except:\n",
    "        error_position.append(count) #иногда попадаются битые ссылки. Здесь мы будем сохранять их позиции\n",
    "        pass\n",
    "    count += 1\n",
    "    \n",
    "print(\"--- Завершено. На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Темы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Inputs of LDA model: Dictionary and Corpus \n",
    "#(doc2bow array of id's of words and their frequency)\n",
    "\n",
    "dct = corpora.Dictionary(prepared_texts)\n",
    "corpus = [dct.doc2bow(item) for item in prepared_texts]\n",
    "#dct.save('news.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('news.model',corpus)\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model and update the model on new data\n",
    "\n",
    "lda_model = LdaMulticore.load(datapath(\"C:/Users/Egran/YandexDisk/Work/news_project/lda_model\"))\n",
    "lda_model.update(corpus)\n",
    "lda_model.save('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train new LDA model\n",
    "\n",
    "lda_daily_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dct,\n",
    "                         random_state=100,\n",
    "                         num_topics=9,\n",
    "                         passes=50,\n",
    "                         chunksize=100,\n",
    "                         batch=False,\n",
    "                         alpha='asymmetric',\n",
    "                         decay=0.5,\n",
    "                         offset=64,\n",
    "                         eta=None,\n",
    "                         eval_every=0,\n",
    "                         iterations=100,\n",
    "                         gamma_threshold=0.001,\n",
    "                         per_word_topics=True)\n",
    "\n",
    "# save the model\n",
    "#lda__daily_model.save('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the topics\n",
    "#lda_model.print_topics(-1)\n",
    "lda_daily_model.show_topics(num_topics=9, num_words=10, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_daily_model.top_topics(texts=prepared_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_daily_model.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_daily_model, texts=prepared_texts, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация выводов тем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод тем по обученной LDA модели\n",
    "# 1. Wordcloud of Top N words in each topic\n",
    "\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=5,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_daily_model.show_topics(formatted=False)\n",
    "#topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20,20), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf на векторах\n",
    "tfidf = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# создание случайной выборки\n",
    "sampling_tfidf = random.choices(corpus_tfidf, k=30)\n",
    "\n",
    "index = similarities.MatrixSimilarity(sampling_tfidf)\n",
    "sims = index[sampling_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sns.heatmap(data=sims, cmap = 'Spectral').set(xticklabels=[], yticklabels=[])\n",
    "plt.title(\"Матрица близости\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more on visuals\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@b.terryjack/nlp-pre-trained-sentiment-analysis-1eb52a9d742c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dct.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерактивная визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = pyLDAvis.gensim.prepare(lda_daily_model, corpus, dct, mds='mmds')\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Саммари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок предобработки текста для рейтера (он аналогичен блоку выше, но возвращает строки, а не отдельные слова)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['china', 'chinese','china\\'s','china’s'] #в стоп список включаем слова, которые являются перекрестными в разных темах\n",
    "lemmatizator = WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def reuter_prepare_text(text, stop = stop_words, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст \n",
    "\n",
    "    parameters:  \n",
    "        stop: список из стоп-слов\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in words if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return ' '.join(words)#words  #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def china_summary_reuters(depth):\n",
    "    \"\"\"\n",
    "    Подготовка резюме по основным новостям по Китаю\n",
    "    depth - количество заголовков для просмотра на странице с последними новостями по Китаю\n",
    "    \"\"\"\n",
    "      \n",
    "    #Выгружаем все свежие заголовки про Китай\n",
    "    r1 = requests.get('https://www.reuters.com/places/china', headers={'Cache-Control': 'no-cache'})\n",
    "    #r1 = requests.get('https://www.reuters.com/search/news?blob=metals')\n",
    "    coverpage =  r1.content\n",
    "    soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "    coverpage_news_r = soup.find_all('h2', class_='FeedItemHeadline_headline')\n",
    "\n",
    "    #Опционально можно вывести отдельно все заголовки:\n",
    "#     for i in range(0, len(coverpage_news_r)):\n",
    "#         print(coverpage_news_r[i].get_text())\n",
    "#         print('///')\n",
    "\n",
    "    # Опционально можно парсить все статьи по существующим заголовкам без ограничения глубины\n",
    "    #number_of_articles = len(coverpage_news_r)\n",
    "    \n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    #замеряем время\n",
    "    start_time = time.time()\n",
    "    print('---Формируется подборка статей---')\n",
    "\n",
    "    #формируем массив статей\n",
    "    for n in range(0, depth):\n",
    "\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news_r[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news_r[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "        try:\n",
    "            x = body[0].find_all('p')\n",
    "        except:\n",
    "            pass\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "    print(\"--- Подборка сформирована. Затрачено %s секунд ---\" % (time.time() - start_time))\n",
    "    \n",
    "    #Проходим по массиву выгруженных новостей и выполняем предобработку\n",
    "    print('---Предобработка---')\n",
    "    prepared_texts = []\n",
    "    for item in news_contents:\n",
    "        prepared_texts.append(reuter_prepare_text(item))\n",
    "    print('---Предобработка выполнена---')\n",
    "    print('///')\n",
    "    #задаем ключевые слова по темам\n",
    "    topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR','trade']\n",
    "\n",
    "    # отбор новостей по ключевым словам и суммаризация\n",
    "    i = 0\n",
    "    for item in prepared_texts:\n",
    "        counter = 0\n",
    "        for word in item.split(' '):\n",
    "            if word in topics_econ:\n",
    "                counter += 1\n",
    "        if counter > 2:\n",
    "            print('Economic story %s' % i)\n",
    "            print(summarize(item, word_count=60))\n",
    "            print('Ссылка на статью:', list_links[i])\n",
    "            print('///')\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Старые эксперименты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выгружаем все свежие заголовки про Китай\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "r1 = requests.get('https://www.reuters.com/places/china', headers={'Cache-Control': 'no-cache'})\n",
    "#r1 = requests.get('https://www.reuters.com/search/news?blob=metals')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('h2', class_='FeedItemHeadline_headline')\n",
    "\n",
    "#Выводим заголовки\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(coverpage_news_r[i].get_text())\n",
    "    print('///')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#задаем ключевые слова по темам\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR','trade']\n",
    "topics_metals = ['metal', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "print(summarize(prepared_texts[0], word_count=60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отбор новостей по ключевым словам\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_econ:\n",
    "            counter += 1\n",
    "    if counter > 2:\n",
    "        print('Economic story %s', i)\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок металлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выгружаем все свежие заголовки про металлы из рейтерс\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "#r1 = requests.get('https://www.reuters.com/places/china', headers={'Cache-Control': 'no-cache'})\n",
    "#https://www.reuters.com/news/archive/metals-news\n",
    "r1 = requests.get('https://www.reuters.com/search/news?blob=Metals&sortBy=date&dateRange=all')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('h3', class_='search-result-title')\n",
    "\n",
    "#Выводим заголовки\n",
    "print('ЗАГОЛОВКИ СТАТЕЙ')\n",
    "print('********************')\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(coverpage_news_r[i].get_text())\n",
    "    print('///')\n",
    "    \n",
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = 'https://www.reuters.com'+coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stopwords_en, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n",
    "\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR']\n",
    "topics_metals = ['base', 'industrial', 'precious', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "\n",
    "print('********************')\n",
    "print('РЕЗЮМЕ СТАТЕЙ')\n",
    "print('********************')\n",
    "previous_item = ''\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_metals:\n",
    "            counter += 1\n",
    "    if counter > 3 and item != previous_item:\n",
    "        print('Metals story %s' %(i))\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n",
    "    previous_item = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Парсинг статей про металлы из архива\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "r1 = requests.get('https://www.reuters.com/news/archive/metals-news')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('div', class_='story-content')\n",
    "\n",
    "#Выводим заголовки\n",
    "print('ЗАГОЛОВКИ СТАТЕЙ')\n",
    "print('********************')\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(re.sub('\\t','',coverpage_news_r[i].get_text()))\n",
    "    print('///')\n",
    "    \n",
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = 'https://www.reuters.com'+coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stopwords_en, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n",
    "\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR']\n",
    "topics_metals = ['base', 'industrial', 'precious', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "\n",
    "print('********************')\n",
    "print('РЕЗЮМЕ СТАТЕЙ')\n",
    "print('********************')\n",
    "previous_item = ''\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_metals:\n",
    "            counter += 1\n",
    "    if counter > 1 and item != previous_item:\n",
    "        print('Metals story %s' %(i))\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n",
    "    previous_item = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Парсинг статей про металлы с сайта Kitco\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "r1 = requests.get('http://www.kitcometals.com/news/')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('td', class_='text')\n",
    "\n",
    "#Выводим заголовки\n",
    "print('ЗАГОЛОВКИ СТАТЕЙ')\n",
    "print('********************')\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(re.sub('\\t','',coverpage_news_r[i].get_text()))    #таким образом убираем тупые пробелы перед заглавием\n",
    "    print('///')\n",
    "    \n",
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('article', itemprop='articleBody')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stopwords_en, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n",
    "\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR']\n",
    "topics_metals = ['base', 'industrial', 'precious', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "\n",
    "print('********************')\n",
    "print('РЕЗЮМЕ СТАТЕЙ')\n",
    "print('********************')\n",
    "previous_item = ''\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_metals:\n",
    "            counter += 1\n",
    "    if counter > 1 and item != previous_item:\n",
    "        print('Metals story %s' %(i))\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n",
    "    previous_item = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
