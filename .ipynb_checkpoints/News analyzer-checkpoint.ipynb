{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from GoogleNews import GoogleNews\n",
    "googlenews = GoogleNews()\n",
    "\n",
    "import newspaper\n",
    "from newspaper import Article\n",
    "from newspaper import fulltext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок выгрузки новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- На формирование массива затрачено 4.9833104610443115 секунд ---\n",
      "---Выгружено 20 статей---\n"
     ]
    }
   ],
   "source": [
    "#Блок выгрузки линков и заголовков новостей из гугла\n",
    "#новости выгружаются по заданной теме, глубина выгрузки - 10 страниц\n",
    "\n",
    "TOPIC = 'China'\n",
    "gnews_links = []\n",
    "gnews=[]\n",
    "googlenews.search(TOPIC)\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(1,3):\n",
    "    googlenews.clear()\n",
    "    googlenews.getpage(i)\n",
    "    for j in range(0, len(googlenews.gettext())):\n",
    "        gnews.append(googlenews.gettext()[j])\n",
    "        gnews_links.append(googlenews.getlinks()[j])\n",
    "\n",
    "print(\"--- На формирование массива затрачено %s секунд ---\" % (time.time() - start_time))             \n",
    "print('---Выгружено %s статей---' % len(gnews_links))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- На выгрузку затрачено 15.410950422286987 секунд ---\n"
     ]
    }
   ],
   "source": [
    "#блок выгрузки новостей и формирования массива текстов\n",
    "body = []\n",
    "count = 0\n",
    "error_position = [] #массив с номерами битых ссылок\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "for url in gnews_links:\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        text = fulltext(html)\n",
    "        body.append(text)\n",
    "    except:\n",
    "        error_position.append(count) #иногда попадаются битые ссылки. Здесь мы будем сохранять их позиции\n",
    "        pass\n",
    "    count += 1\n",
    "    \n",
    "print(\"--- На выгрузку затрачено %s секунд ---\" % (time.time() - start_time))             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок предобработки текста\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words = stop_words + ['china', 'chinese','china\\'s','china’s'] #в стоп список включаем слова, которые являются перекрестными в разных темах\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stop_words, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in words if item not in stop]\n",
    "    words = [item for item in words if len(item)>3]\n",
    "    \n",
    "    return words#' '.join(words) #здесь выбираем формат вывода: в виде отдельных токенов или в форме связной строки-текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in body:\n",
    "    prepared_texts.append(prepare_text(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Inputs of LDA model: Dictionary and Corpus\n",
    "\n",
    "dct = corpora.Dictionary(prepared_texts)\n",
    "corpus = [dct.doc2bow(line) for line in prepared_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model and update the model on new data\n",
    "\n",
    "lda_model = LdaMulticore.load(datapath(\"C:/Users/Egran/YandexDisk/Work/news_project/lda_model\"))\n",
    "lda_model.update(corpus)\n",
    "lda_model.save('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train new LDA model\n",
    "\n",
    "# lda_model = LdaMulticore(corpus=corpus,\n",
    "#                          id2word=dct,\n",
    "#                          random_state=100,\n",
    "#                          num_topics=15,\n",
    "#                          passes=10,\n",
    "#                          chunksize=1000,\n",
    "#                          batch=False,\n",
    "#                          alpha='asymmetric',\n",
    "#                          decay=0.5,\n",
    "#                          offset=64,\n",
    "#                          eta=None,\n",
    "#                          eval_every=0,\n",
    "#                          iterations=100,\n",
    "#                          gamma_threshold=0.001,\n",
    "#                          per_word_topics=True)\n",
    "\n",
    "# # save the model\n",
    "# lda_model.save('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14,\n",
       "  '0.001*\"headcount:\" + 0.001*\"headquarters:\" + 0.000*\"tongdun\" + 0.000*\"start-up\" + 0.000*\"technology\" + 0.000*\"u.s.\" + 0.000*\"trade\" + 0.000*\"company\" + 0.000*\"autonomous\" + 0.000*\"start-ups\"'),\n",
       " (13,\n",
       "  '0.002*\"airport\" + 0.001*\"daxing\" + 0.001*\"flight\" + 0.001*\"passenger\" + 0.001*\"airline\" + 0.001*\"terminal\" + 0.000*\"beijing\" + 0.000*\"image\" + 0.000*\"tenant\" + 0.000*\"aviation\"'),\n",
       " (12,\n",
       "  '0.005*\"said\" + 0.004*\"trade\" + 0.004*\"also\" + 0.004*\"u.s.\" + 0.003*\"would\" + 0.003*\"year\" + 0.003*\"could\" + 0.003*\"beijing\" + 0.003*\"company\" + 0.002*\"said.\"'),\n",
       " (11,\n",
       "  '0.000*\"said\" + 0.000*\"navy\" + 0.000*\"trade\" + 0.000*\"business\" + 0.000*\"said.\" + 0.000*\"could\" + 0.000*\"u.s.\" + 0.000*\"hong\" + 0.000*\"yuan\" + 0.000*\"also\"'),\n",
       " (10,\n",
       "  '0.000*\"biden\" + 0.000*\"hunter\" + 0.000*\"said\" + 0.000*\"xiaoguang,\" + 0.000*\"depiction\" + 0.000*\"studio\" + 0.000*\"china,\" + 0.000*\"yat-sen,\" + 0.000*\"hang\" + 0.000*\"artist\"'),\n",
       " (4,\n",
       "  '0.001*\"informed\" + 0.001*\"u.s.\" + 0.001*\"johnson\" + 0.001*\"trade\" + 0.000*\"spot\" + 0.000*\"dominate\" + 0.000*\"analysis\" + 0.000*\"reporting,\" + 0.000*\"independent\" + 0.000*\"corporate,\"'),\n",
       " (3,\n",
       "  '0.002*\"mosque\" + 0.002*\"religious\" + 0.001*\"emily\" + 0.001*\"feng/npr\" + 0.001*\"islamic\" + 0.001*\"imam\" + 0.001*\"school\" + 0.001*\"muslim\" + 0.001*\"ningxia\" + 0.001*\"henan\"'),\n",
       " (2,\n",
       "  '0.002*\"philippine\" + 0.001*\"nanyuan\" + 0.001*\"said\" + 0.001*\"island\" + 0.001*\"gambling\" + 0.001*\"airport\" + 0.001*\"temple\" + 0.001*\"mazu\" + 0.001*\"state\" + 0.001*\"google\"'),\n",
       " (1,\n",
       "  '0.001*\"douyu\" + 0.001*\"douyu’s\" + 0.001*\"virtual\" + 0.001*\"streaming\" + 0.001*\"million\" + 0.001*\"zhihu\" + 0.001*\"league\" + 0.001*\"platform\" + 0.001*\"streamer\" + 0.001*\"tencent\"'),\n",
       " (0,\n",
       "  '0.002*\"browser\" + 0.002*\"javascript\" + 0.002*\"cooky\" + 0.002*\"make\" + 0.002*\"sure\" + 0.002*\"support\" + 0.002*\"blocking\" + 0.002*\"please\" + 0.002*\"loading.\" + 0.002*\"service\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the topics\n",
    "lda_model.print_topics(num_topics=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/@b.terryjack/nlp-pre-trained-sentiment-analysis-1eb52a9d742c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выгружаем все свежие заголовки про Китай\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "r1 = requests.get('https://www.reuters.com/places/china', headers={'Cache-Control': 'no-cache'})\n",
    "#r1 = requests.get('https://www.reuters.com/search/news?blob=metals')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('h2', class_='FeedItemHeadline_headline')\n",
    "\n",
    "#Выводим заголовки\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(coverpage_news_r[i].get_text())\n",
    "    print('///')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stopwords_en, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#задаем ключевые слова по темам\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR','trade']\n",
    "topics_metals = ['metal', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "print(summarize(prepared_texts[0], word_count=60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отбор новостей по ключевым словам\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_econ:\n",
    "            counter += 1\n",
    "    if counter > 2:\n",
    "        print('Economic story %s', i)\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок металлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выгружаем все свежие заголовки про металлы из рейтерс\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "#r1 = requests.get('https://www.reuters.com/places/china', headers={'Cache-Control': 'no-cache'})\n",
    "#https://www.reuters.com/news/archive/metals-news\n",
    "r1 = requests.get('https://www.reuters.com/search/news?blob=Metals&sortBy=date&dateRange=all')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('h3', class_='search-result-title')\n",
    "\n",
    "#Выводим заголовки\n",
    "print('ЗАГОЛОВКИ СТАТЕЙ')\n",
    "print('********************')\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(coverpage_news_r[i].get_text())\n",
    "    print('///')\n",
    "    \n",
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = 'https://www.reuters.com'+coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stopwords_en, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n",
    "\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR']\n",
    "topics_metals = ['base', 'industrial', 'precious', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "\n",
    "print('********************')\n",
    "print('РЕЗЮМЕ СТАТЕЙ')\n",
    "print('********************')\n",
    "previous_item = ''\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_metals:\n",
    "            counter += 1\n",
    "    if counter > 3 and item != previous_item:\n",
    "        print('Metals story %s' %(i))\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n",
    "    previous_item = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Парсинг статей про металлы из архива\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "r1 = requests.get('https://www.reuters.com/news/archive/metals-news')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('div', class_='story-content')\n",
    "\n",
    "#Выводим заголовки\n",
    "print('ЗАГОЛОВКИ СТАТЕЙ')\n",
    "print('********************')\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(re.sub('\\t','',coverpage_news_r[i].get_text()))\n",
    "    print('///')\n",
    "    \n",
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = 'https://www.reuters.com'+coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('div', class_='StandardArticleBody_body')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stopwords_en, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n",
    "\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR']\n",
    "topics_metals = ['base', 'industrial', 'precious', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "\n",
    "print('********************')\n",
    "print('РЕЗЮМЕ СТАТЕЙ')\n",
    "print('********************')\n",
    "previous_item = ''\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_metals:\n",
    "            counter += 1\n",
    "    if counter > 1 and item != previous_item:\n",
    "        print('Metals story %s' %(i))\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n",
    "    previous_item = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Парсинг статей про металлы с сайта Kitco\n",
    "# s = requests.session()\n",
    "# s.cookies.clear()\n",
    "\n",
    "r1 = requests.get('http://www.kitcometals.com/news/')\n",
    "coverpage =  r1.content\n",
    "soup = BeautifulSoup(coverpage, 'html5lib')\n",
    "coverpage_news_r = soup.find_all('td', class_='text')\n",
    "\n",
    "#Выводим заголовки\n",
    "print('ЗАГОЛОВКИ СТАТЕЙ')\n",
    "print('********************')\n",
    "for i in range(0, len(coverpage_news_r)):\n",
    "    print(re.sub('\\t','',coverpage_news_r[i].get_text()))    #таким образом убираем тупые пробелы перед заглавием\n",
    "    print('///')\n",
    "    \n",
    "# Парсим все статьи по существующим заголовкам\n",
    "number_of_articles = len(coverpage_news_r)\n",
    "# Empty lists for content, links and titles\n",
    "news_contents = []\n",
    "list_links = []\n",
    "list_titles = []\n",
    "\n",
    "#замеряем время\n",
    "start_time = time.time()\n",
    "\n",
    "#формируем массив с корпусами статей\n",
    "for n in np.arange(0, number_of_articles):\n",
    "    \n",
    "    \n",
    "    # Getting the link of the article\n",
    "    link = coverpage_news_r[n].find('a')['href']\n",
    "    list_links.append(link)\n",
    "    \n",
    "    # Getting the title\n",
    "    title = coverpage_news_r[n].find('a').get_text()\n",
    "    list_titles.append(title)\n",
    "    \n",
    "    # Reading the content (it is divided in paragraphs)\n",
    "    article = requests.get(link)\n",
    "    article_content = article.content\n",
    "    soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "    body = soup_article.find_all('article', itemprop='articleBody')\n",
    "    try:\n",
    "        x = body[0].find_all('p')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Unifying the paragraphs\n",
    "    list_paragraphs = []\n",
    "    for p in np.arange(0, len(x)):\n",
    "        paragraph = x[p].get_text()\n",
    "        list_paragraphs.append(paragraph)\n",
    "        final_article = \" \".join(list_paragraphs)\n",
    "        \n",
    "    news_contents.append(final_article)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "#Блок предобработки текста\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "lemmatizator = nltk.stem.WordNetLemmatizer()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "#функция предобработки\n",
    "def prepare_text(text, stop = stopwords_en, tokenizer = tokenizer):\n",
    "    \"\"\" \n",
    "    Возвращает тексты: \n",
    "        * лемматизированные,\n",
    "        * без стоп-слов, \n",
    "        * в нижнем регистре, \n",
    "        * все слова длиннее 3 символов\n",
    "\n",
    "    text: string\n",
    "        Текст поста\n",
    "\n",
    "    parameters: list \n",
    "        stop: список из стоп-слов, example: ['а', политик', 'выбирать']\n",
    "        tokenizer: токенизатор для текста, поданного на вход\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    #words = [lemmatizator.lemmatize(token) for token in tokens]\n",
    "    words = [item for item in tokens if item not in stop]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "#Проходим по корпусу выгруженных новостей и выполняем предобработку всего корпуса\n",
    "prepared_texts = []\n",
    "for item in news_contents:\n",
    "    prepared_texts.append(prepare_text(item))\n",
    "\n",
    "topics_econ = ['economy', 'inflation', 'cpi', 'gdp', 'output', 'pboc', 'central bank', 'money market', 'RRR']\n",
    "topics_metals = ['base', 'industrial', 'precious', 'metals', 'mining', 'coal', 'ore', 'iron', 'gold', 'platinum', 'copper', 'silver']\n",
    "\n",
    "print('********************')\n",
    "print('РЕЗЮМЕ СТАТЕЙ')\n",
    "print('********************')\n",
    "previous_item = ''\n",
    "i = 0\n",
    "for item in prepared_texts:\n",
    "    counter = 0\n",
    "    for word in item.split(' '):\n",
    "        if word in topics_metals:\n",
    "            counter += 1\n",
    "    if counter > 1 and item != previous_item:\n",
    "        print('Metals story %s' %(i))\n",
    "        print(summarize(item, word_count=60))\n",
    "        print('Ссылка на статью:', list_links[i])\n",
    "        print('///')\n",
    "        i += 1\n",
    "    previous_item = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
